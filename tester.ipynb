{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mdone.               \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Importing Modules'''\n",
    "\n",
    "'''\n",
    "Prerequisites:\n",
    "NumPy             https://numpy.org/doc/stable/\n",
    "Matplotlib        https://matplotlib.org/stable/index.html\n",
    "PyTorch           https://pytorch.org/docs/stable/index.html\n",
    "Torchvision       https://pytorch.org/docs/stable/index.html\n",
    "PIL               https://pillow.readthedocs.io/en/stable/\n",
    "GitPython         https://gitpython.readthedocs.io/en/stable/\n",
    "split-folders:    https://pypi.org/project/split-folders/\n",
    "python-dotenv:    https://pypi.org/project/python-dotenv/\n",
    "'''\n",
    "\n",
    "# vanilla:\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from time import strptime\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import sys\n",
    "# external:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import PIL.Image as Image\n",
    "import git\n",
    "import splitfolders\n",
    "from dotenv import load_dotenv\n",
    "# custom:\n",
    "from routines import *\n",
    "from displays import *\n",
    "import myTransforms\n",
    "import consts\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mdone.               \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Parsing the .env File'''\n",
    "\n",
    "\n",
    "# Loading sensitive info from the dotenv file.\n",
    "#  These are needed in order to clone the data repo:\n",
    "if not load_dotenv(consts.dotenv_path):\n",
    "    e_msg = 'cannot find the required .env file'\n",
    "    raise SystemExit(e_msg)\n",
    "gh_token = os.getenv('GH_TOKEN')\n",
    "gh_username = os.getenv('GH_USERNAME')\n",
    "repo_name = os.getenv('REMOTE_REPO_NAME')\n",
    "repo_url = f'https://{gh_token}@github.com/{gh_username}/{repo_name}.git'\n",
    "dataset_path = repo_name + '\\\\dataset'\n",
    "classes_path = repo_name + '\\\\classes.json'\n",
    "url_issue = False\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mdone.               \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Cloning the Remote Data-Repository'''\n",
    "\n",
    "\n",
    "# Checks if a leftover repo exists, overwrite it if so:\n",
    "if os.path.exists(repo_name):\n",
    "    git.rmtree(repo_name)\n",
    "# Clones the repo, and raises an exception if the remote URL is corrupted:\n",
    "try:\n",
    "    git.Repo.clone_from(repo_url, repo_name)\n",
    "except Exception as e:\n",
    "    url_issue = True\n",
    "    pass\n",
    "if url_issue:\n",
    "    e_msg = 'there is an issue with the remote repo URL'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtotal classes       \u001b[0m\n",
      "\u001b[0m\u001b[0m15                  \u001b[0m\n",
      "\u001b[32mdone.               \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Parsing the JSON File from the Data Repository'''\n",
    "\n",
    "\n",
    "json_not_found = False\n",
    "try:\n",
    "    with open(classes_path, 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "        classes = dict(json_file[0])\n",
    "    # Displays the JSON file metadata:\n",
    "    println(['total classes'], header=True)\n",
    "    println([len(classes)])\n",
    "except FileNotFoundError as e:\n",
    "    json_not_found=True\n",
    "if json_not_found:\n",
    "    e_msg=f'cannot locate the \\'classes.json\\' file in \"{repo_name}\".'\\\n",
    "        + f'\\nre-run the \\'Cloning the Remote Data-Repositoryg\\' cell and try again.'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mlooking for image files in './input/'...\u001b[0m\n",
      "\u001b[1m#                   \u001b[1mfilename            \u001b[1mheight              \u001b[1mwidth               \u001b[1mcolor               \u001b[0m\n",
      "\u001b[0m\u001b[0m1                   \u001b[0m\u001b[0m1.jpg               \u001b[0m\u001b[0m900                 \u001b[0m\u001b[0m1200                \u001b[32mrgb                 \u001b[0m\n",
      "\u001b[0m\u001b[0m2                   \u001b[0m\u001b[0m2.png               \u001b[0m\u001b[0m650                 \u001b[0m\u001b[0m1169                \u001b[32mrgb                 \u001b[0m\n",
      "\u001b[0m\u001b[0m3                   \u001b[0m\u001b[0m3.png               \u001b[0m\u001b[0m427                 \u001b[0m\u001b[0m640                 \u001b[31mgreyscale           \u001b[0m\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "cannot parse greyscale images",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m cannot parse greyscale images\n"
     ]
    }
   ],
   "source": [
    "'''Parsing the Input Directory Supplied by the User'''\n",
    "\n",
    "\n",
    "println([(f'looking for image files in \\'{consts.input_path}\\'...', 'y')])\n",
    "\n",
    "files = []\n",
    "bad_files = []\n",
    "no_input_dir = False\n",
    "no_files = False\n",
    "got_greyscale = False\n",
    "\n",
    "try:\n",
    "    for file in os.listdir(consts.input_path):\n",
    "        if file.endswith(consts.valid_filetypes):\n",
    "            files.append(file)\n",
    "        else:\n",
    "            bad_files.append(file)\n",
    "except FileNotFoundError as e:\n",
    "    no_input_dir = True\n",
    "if no_input_dir:\n",
    "    e_msg = 'cannot find the input directory (it must be named \\'input\\' and be in root)'\n",
    "    raise SystemExit(e_msg)\n",
    "if bad_files != []:\n",
    "    e_msg = f'invalid files were found in input the directory, please remove them:\\n{bad_files}'\n",
    "    raise SystemExit(e_msg)\n",
    "if files == []:\n",
    "    e_msg = 'cannot find any image files in the input directory'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "# Displaying:\n",
    "println(['#', 'filename', 'height', 'width', 'color'], header=True)    \n",
    "for idx, file in enumerate(files):\n",
    "    img = Image.open(consts.input_path + file)\n",
    "    tensor = transforms.Compose([transforms.PILToTensor()])(img)\n",
    "    if len(tensor) == 3:\n",
    "        color = ('rgb', 'g')\n",
    "    else:\n",
    "        color = ('greyscale', 'r')\n",
    "        got_greyscale = True\n",
    "    color = ('rgb', 'g') if len(tensor)==3 else ('greyscale', 'r')\n",
    "    height = len(tensor[0])\n",
    "    width = len(tensor[0][0])\n",
    "    println([idx + 1, file, height, width, color])\n",
    "if got_greyscale:\n",
    "    e_msg = 'cannot parse greyscale images, revise your input'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mlooking for '.pth' files in './checkpoints' to instanctiate a new model...\u001b[0m\n",
      "\u001b[32mdone.               \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Instanciating Our Model'''\n",
    "\n",
    "\n",
    "println([(f'looking for \\'.pth\\' files in \\'{consts.checkpoints_path}\\' to instanctiate a new model...', 'y')])\n",
    "\n",
    "checkpoints = []\n",
    "no_checkpoints_dir = False\n",
    "\n",
    "try:\n",
    "    for file in os.listdir(consts.checkpoints_path):\n",
    "        if file.endswith('.pth'):\n",
    "            checkpoints.append(file)\n",
    "except FileNotFoundError as e:\n",
    "    no_checkpoints_dir = True\n",
    "\n",
    "if no_checkpoints_dir:\n",
    "    e_msg = f'cannot find the checkpoints dir: \\'{consts.checkpoints_path}\\''\n",
    "    raise SystemExit(e_msg)\n",
    "if checkpoints == []:\n",
    "    e_msg = f'cannot find any \\'.pth\\' files in \\'{consts.checkpoints_path}\\''\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "latest_checkpoint = latestCheckpoint()\n",
    "pretrained = True\n",
    "weights=('DEFAULT' if pretrained else None)\n",
    "model = loadCheckpoint(latest_checkpoint, weights=weights)\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creating the Images List'''\n",
    "\n",
    "\n",
    "images = [] # list of tuples: (filename, Tensor)\n",
    "\n",
    "for file in files:\n",
    "    pil_image = Image.open(consts.input_path + '//' + file)\n",
    "    pil_image = myTransforms.pilimg_transforms(pil_image)\n",
    "    images.append((file, pil_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m#                   \u001b[1mfilename            \u001b[1mprediction          \u001b[0m\n",
      "\u001b[0m\u001b[0m1                   \u001b[0m\u001b[0m1.jpg               \u001b[0m\u001b[0mUSA                 \u001b[0m\n",
      "\u001b[0m\u001b[0m2                   \u001b[0m\u001b[0m2.png               \u001b[0m\u001b[0mIsrael              \u001b[0m\n",
      "\u001b[32mdone.               \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Prediciting the Supplied Images'''\n",
    "\n",
    "\n",
    "predictions = [] # list of tuples: (filename, prediction)\n",
    "\n",
    "for filename, img in images:\n",
    "    inputs = Variable(img)\n",
    "    inputs = inputs.unsqueeze(dim = 0)\n",
    "    log_probabilities = model.forward(inputs)\n",
    "    probabilities = torch.exp(log_probabilities)\n",
    "    top_probabilities, top_classes = probabilities.topk(15, dim=1)\n",
    "    pred = top_classes[0][0]\n",
    "    pred = str(int(pred) + 1)\n",
    "    predictions.append((filename, classes[pred]))\n",
    "\n",
    "println(['#', 'filename', 'prediction'], header=True)\n",
    "for idx, pred in enumerate(predictions):\n",
    "    println([idx + 1,\n",
    "            pred[0],\n",
    "            pred[1].upper() if pred[1] in ['uk','usa'] else pred[1].capitalize()])\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generating the Results Directory'''\n",
    "\n",
    "\n",
    "# If an output dir already exists, overwrite it:\n",
    "if os.path.exists(consts.output_path):\n",
    "    shutil.rmtree(consts.output_path)\n",
    "os.mkdir(consts.output_path)\n",
    "\n",
    "for pred in predictions:\n",
    "    # Creates a new class subdir to dump the image in, if one does not exist yet:\n",
    "    filename = pred[0]\n",
    "    classname = pred[1]\n",
    "\n",
    "    class_path = consts.output_path + classname + '/'\n",
    "\n",
    "    if not os.path.exists(class_path):\n",
    "        os.mkdir(class_path)\n",
    "    shutil.copyfile(consts.input_path + filename, class_path + filename)\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
