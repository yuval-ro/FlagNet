{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4dfb1-584f-4c6e-b39a-c32fc1863ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla modules:\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from time import strptime\n",
    "import datetime\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "# external modules:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import PIL.Image as Image\n",
    "import git\n",
    "import splitfolders\n",
    "\n",
    "# Prerequisites:\n",
    "# NumPy             https://numpy.org/doc/stable/\n",
    "# Matplotlib        https://matplotlib.org/stable/index.html\n",
    "# PyTorch           https://pytorch.org/docs/stable/index.html\n",
    "# Torchvision       https://pytorch.org/docs/stable/index.html\n",
    "# PIL               https://pillow.readthedocs.io/en/stable/\n",
    "# GitPython         https://gitpython.readthedocs.io/en/stable/\n",
    "# split-folders:    https://pypi.org/project/split-folders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2aa396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers:\n",
    "class Style:\n",
    "\tBOLD = \t\t'\\033[1m'\n",
    "\tBLACK = \t'\\033[30m'\n",
    "\tRED =\t\t'\\033[31m'\n",
    "\tGREEN =\t\t'\\033[32m'\n",
    "\tYELLOW =\t'\\033[33m'\n",
    "\tBLUE =\t\t'\\033[34m'\n",
    "\tMAGENTA =\t'\\033[35m'\n",
    "\tCYAN =\t\t'\\033[36m'\n",
    "\tWHITE =\t\t'\\033[37m'\n",
    "\tEND =\t\t'\\033[0m'\n",
    "\n",
    "def color(input, color, bold=False):\n",
    "\tif   color in ['k', 'black']:\n",
    "\t\treturn (Style.BLACK + str(input) + Style.END) if not bold else (Style.BOLD + Style.BLACK + str(input) + Style.END)\n",
    "\telif color in ['r', 'red']:\n",
    "\t\treturn Style.RED + str(input) + Style.END if not bold else (Style.BOLD + Style.RED + str(input) + Style.END)\n",
    "\telif color in ['g', 'green']:\n",
    "\t\treturn Style.GREEN + str(input) + Style.END if not bold else (Style.BOLD + Style.GREEN + str(input) + Style.END)\n",
    "\telif color in ['y', 'yellow']:\n",
    "\t\treturn Style.YELLOW + str(input) + Style.END if not bold else (Style.BOLD + Style.YELLOW + str(input) + Style.END)\n",
    "\telif color in ['b', 'blue']:\n",
    "\t\treturn Style.BLUE + str(input) + Style.END if not bold else (Style.BOLD + Style.BLUE + str(input) + Style.END)\n",
    "\telif color in ['m', 'magenta']:\n",
    "\t\treturn Style.MAGENTA + str(input) + Style.END if not bold else (Style.BOLD + Style.MAGENTA + str(input) + Style.END)\n",
    "\telif color in ['c', 'cyan']:\n",
    "\t\treturn Style.CYAN + str(input) + Style.END if not bold else (Style.BOLD + Style.CYAN + str(input) + Style.END)\n",
    "\telif color in ['w', 'white']:\n",
    "\t\treturn Style.WHITE + str(input) + Style.END if not bold else (Style.BOLD + Style.WHITE + str(input) + Style.END)\n",
    "\telse:\n",
    "\t\traise SystemExit(f'invalid parameters')\n",
    "\n",
    "def println(strings, bold=False):\n",
    "\tfor i, s in enumerate(strings):\n",
    "\t\tif i == 0 and bold:\n",
    "\t\t\tprint(Style.BOLD+'{0:<20}'.format(s), end='')\n",
    "\t\telif i == (len(strings) - 1):\n",
    "\t\t\tif bold:\n",
    "\t\t\t\tprint('{0:<20}'.format(s)+Style.END)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint('{0:<20}'.format(s))\n",
    "\t\telse:\n",
    "\t\t\tprint('{0:<20}'.format(s), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5fa87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the data repository from GitHub:\n",
    "token = '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'\n",
    "username = 'yuval-ro'\n",
    "remote_repo = 'FlagNet_data'\n",
    "local_repo = remote_repo\n",
    "remote_repo_url = f'https://{token}@github.com/{username}/{remote_repo}.git'\n",
    "\n",
    "try:\n",
    "    git.Repo.clone_from(remote_repo_url, local_repo)\n",
    "except:\n",
    "    git.rmtree(local_repo)\n",
    "    git.Repo.clone_from(remote_repo_url, local_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a41e81-7b80-4b39-be79-c40461103f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset sanity check and display:\n",
    "IMAGES_PER_CLASS = 30\n",
    "dataset_dir = local_repo + '\\\\dataset'\n",
    "classes_json = local_repo + '\\\\classes.json'\n",
    "images_per_class = []\n",
    "bad_dirs = []\n",
    "json_ne_dirs = False\n",
    "\n",
    "# Locates and parses the \"classes.json\" file for label mapping:\n",
    "with open(classes_json, 'r') as f:\n",
    "    classes = OrderedDict(json.load(f))\n",
    "    dir_names = [dataset_dir + '\\\\%.2d' % i for i in range(1, len(classes) + 1)]\n",
    "\n",
    "\n",
    "# Checks the number of classes defined in the json equal to number of classes subdirs:\n",
    "if len(os.listdir(dataset_dir)) != len(classes):\n",
    "    json_ne_dirs=True\n",
    "\n",
    "# Checks the number of images of each class subdir:\n",
    "for dir_tuple in os.walk(dataset_dir):\n",
    "    if dir_tuple[0] in dir_names: # skips junk directories\n",
    "        images_in_dir = len(dir_tuple[2])\n",
    "        images_per_class.append(images_in_dir)\n",
    "        if images_in_dir != IMAGES_PER_CLASS:\n",
    "            bad_dirs.append(dir_tuple[0])\n",
    "\n",
    "# Displays the metadata nicely:\n",
    "println(['id', 'class', 'images'], bold=True)\n",
    "\n",
    "for i, (ID, Class) in enumerate(classes.items()):\n",
    "    println([ID,\n",
    "            Class.upper() if Class in ['uk','usa'] else Class.capitalize(),\n",
    "            images_per_class[i] if images_per_class[i] == IMAGES_PER_CLASS else color(images_per_class[i], 'r')])\n",
    "println(['','','total'], bold=True)\n",
    "\n",
    "println(['','',(sum(images_per_class)) if sum(images_per_class) == (len(classes) * IMAGES_PER_CLASS) else color(sum(images_per_class), 'r')])\n",
    "\n",
    "# Throws exceptions if needed:\n",
    "if json_ne_dirs:\n",
    "    raise SystemExit(f'number of classes according to the json file ({len(classes)}) does not correlate with total dirs ({len(os.listdir(dataset_dir))}) in \\\"{dataset_dir}\\\"')\n",
    "elif bad_dirs != []:\n",
    "    raise SystemExit(f'image count in the following directories is incorrect: {bad_dirs}')\n",
    "else:\n",
    "    println(['','',color('all okay!', 'g')], bold=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image pre-processing hyperparameters:\n",
    "degrees_rotation = 30\n",
    "size_crop = 224\n",
    "size_resize = 256\n",
    "normalize_mean = [0.485, 0.456, 0.406]\n",
    "normalize_std = [0.229, 0.224, 0.225]\n",
    "batch_size = 32\n",
    "\n",
    "# Define a transform for each set:\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(degrees_rotation),\n",
    "                                       transforms.RandomResizedCrop(size_crop),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(normalize_mean, normalize_std)\n",
    "                                      ])\n",
    "\n",
    "valid_transforms = transforms.Compose([transforms.Resize(size_resize), \n",
    "                                       transforms.CenterCrop(size_crop),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(normalize_mean, normalize_std)\n",
    "                                      ])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(size_resize),\n",
    "                                      transforms.CenterCrop(size_crop),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(normalize_mean, normalize_std)\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a462ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the dataset into three folders with the following ratio of images:\n",
    "ratio = (.8, .1, .1) # Defined ratio of set size\n",
    "sets_path = 'sets'\n",
    "train_set_path = sets_path + '\\\\train'\n",
    "valid_set_path = sets_path + '\\\\val'\n",
    "test_set_path = sets_path + '\\\\test'\n",
    "\n",
    "if os.path.exists(sets_path):\n",
    "    shutil.rmtree(sets_path)\n",
    "\n",
    "splitfolders.ratio(\n",
    "    dataset_dir,\n",
    "    output=sets_path,\n",
    "    seed=1337,\n",
    "    ratio=ratio,\n",
    "    group_prefix=None,\n",
    "    move=False)\n",
    "\n",
    "# Instanciate each set:\n",
    "train_data = datasets.ImageFolder(train_set_path, transform = train_transforms)\n",
    "valid_data = datasets.ImageFolder(valid_set_path, transform = valid_transforms)\n",
    "test_data = datasets.ImageFolder(test_set_path, transform = test_transforms)\n",
    "\n",
    "# Create the DataLoader for each set:\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the model (VGG16) and define a classifier for it:\n",
    "model = models.vgg16(pretrained = False)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "dropout_probability = 0.5\n",
    "in_features = 25088\n",
    "out_features = 1024\n",
    "\n",
    "classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(in_features, out_features)),\n",
    "                                        ('drop', nn.Dropout(p = dropout_probability)),\n",
    "                                        ('relu', nn.ReLU()),\n",
    "                                        ('fc2', nn.Linear(out_features, len(classes))),\n",
    "                                        ('output', nn.LogSoftmax(dim = 1))\n",
    "                                       ]))\n",
    "\n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for loading a checkpoint:\n",
    "def load_checkpoint(file_path):\n",
    "    checkpoint = torch.load(file_path)\n",
    "    learning_rate = checkpoint['learning_rate']\n",
    "    model = getattr(torchvision.models, checkpoint['network'])(pretrained=True)\n",
    "    model.classifier = checkpoint['classifier']\n",
    "    model.epochs = checkpoint['epochs']\n",
    "    model.optimizer = checkpoint['optimizer']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Sort all checkpoints by their filename (date created, ascending order):\n",
    "timestamp_format = '%H%M%S_%d%m%y'\n",
    "checkpoints = []\n",
    "for i in os.listdir():\n",
    "    if i.endswith('.pth'):\n",
    "        filename, ext = os.path.splitext(i)\n",
    "        filename_date_tuple = (time.strptime(filename, timestamp_format), i)\n",
    "        checkpoints.append(filename_date_tuple)\n",
    "\n",
    "\n",
    "# If no .pth file were found raise an exception:\n",
    "if checkpoints == []:\n",
    "    raise SystemExit(f'no .pth were found whilst trying to load a checkpoint')\n",
    "\n",
    "\n",
    "# Get the latest checkpoint and load it onto the model instance\n",
    "latest_checkpoint = (sorted(checkpoints, key=lambda x: x[0])[-1])[1]\n",
    "model = load_checkpoint(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model:\n",
    "\n",
    "# Use GPU if it's available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "learning_rate = .001\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr = learning_rate)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "validation_step = True\n",
    "\n",
    "print('Training started')\n",
    "start_training_time = time.time()\n",
    "\n",
    "println(['epoch', 'train loss', 'valid loss', 'accuracy'], bold=True)\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for inputs, labels in train_loader:     \n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        log_probabilities = model.forward(inputs)\n",
    "        loss = criterion(log_probabilities, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = train_loss + loss.item()\n",
    "    \n",
    "    if validation_step:\n",
    "        valid_loss = 0\n",
    "        valid_accuracy = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                log_probabilities = model.forward(inputs)\n",
    "                loss = criterion(log_probabilities, labels)\n",
    "                valid_loss = valid_loss + loss.item()\n",
    "                # Calculate accuracy\n",
    "                probabilities = torch.exp(log_probabilities)\n",
    "                top_probability, top_class = probabilities.topk(1, dim = 1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                valid_accuracy = valid_accuracy + torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        model.train()\n",
    "       \n",
    "    println([\n",
    "        (epoch + 1),\n",
    "        '%.3f'%(train_loss / len(train_loader)),\n",
    "        '%.3f'%(valid_loss / len(valid_loader)),\n",
    "        '%.3f'%(valid_accuracy / len(valid_loader))])\n",
    "        \n",
    "end_training_time = time.time()\n",
    "print('Training ended')\n",
    "\n",
    "training_time = end_training_time - start_training_time\n",
    "print('\\nTraining time: {:.0f}m {:.0f}s'.format(training_time / 60, training_time % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cbc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the model:\n",
    "\n",
    "test_loss = 0\n",
    "test_accuracy = 0\n",
    "model.eval()\n",
    "\n",
    "\n",
    "print('Validation started')\n",
    "start_time = time.time()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    log_probabilities = model.forward(inputs)\n",
    "    loss = criterion(log_probabilities, labels)\n",
    "\n",
    "    test_loss = test_loss + loss.item()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    probabilities = torch.exp(log_probabilities)\n",
    "    top_probability, top_class = probabilities.topk(1, dim = 1)\n",
    "\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "\n",
    "    test_accuracy = test_accuracy + torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "end_time = time.time()\n",
    "print('Validation ended')\n",
    "validation_time = end_time - start_time\n",
    "\n",
    "print('Validation time: {:.0f}m {:.0f}s'.format(validation_time / 60, validation_time % 60))\n",
    "\n",
    "println(['type', 'loss', 'accuracy'], bold=True)\n",
    "println([\n",
    "    'validation',\n",
    "    '%.3f'%(test_loss / len(test_loader)),\n",
    "    '%.3f'%(test_accuracy / len(test_loader))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the checkpoint:\n",
    "timestamp_format = '%H%M%S_%d%m%y'\n",
    "timestamp = datetime.datetime.now().strftime(timestamp_format)\n",
    "checkpoint_name = f'{timestamp}.pth'\n",
    "\n",
    "model.class_to_idx = train_data.class_to_idx\n",
    "\n",
    "checkpoint = {'network': 'vgg16',\n",
    "              'input_size': in_features,\n",
    "              'output_size': len(classes),\n",
    "              'learning_rate': learning_rate,       \n",
    "              'batch_size': batch_size,\n",
    "              'classifier' : classifier,\n",
    "              'epochs': epochs,\n",
    "              'optimizer': optimizer.state_dict(),\n",
    "              'state_dict': model.state_dict(),\n",
    "              'class_to_idx': model.class_to_idx}\n",
    "\n",
    "torch.save(checkpoint, checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(pil_image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    \n",
    "    img_loader = transforms.Compose([transforms.Resize(size_resize),\n",
    "                                     transforms.CenterCrop(size_crop), \n",
    "                                     transforms.ToTensor()])\n",
    "    \n",
    "    #pil_image = Image.open(image)\n",
    "    pil_image = img_loader(pil_image).float()\n",
    "    \n",
    "    np_image = np.array(pil_image)    \n",
    "    \n",
    "    mean = np.array(normalize_mean)\n",
    "    std = np.array(normalize_std)\n",
    "    np_image = (np.transpose(np_image, (1, 2, 0)) - mean) / std    \n",
    "    np_image = np.transpose(np_image, (2, 0, 1))\n",
    "            \n",
    "    return np_image\n",
    "\n",
    "def imshow(np_image, ax = None, title = None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    np_image = np.transpose(np_image, (1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array(normalize_mean)\n",
    "    std = np.array(normalize_std)\n",
    "    np_image = std * np_image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    np_image = np.clip(np_image, 0, 1)\n",
    "    \n",
    "    ax.imshow(np_image)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "images_paths = result = [os.path.join(dp, f) for dp, dn, filenames in os.walk(sets_path) for f in filenames if os.path.splitext(f)[1] == '.jpg']\n",
    "random.seed(10)\n",
    "random_image_path = random.choice(images_paths)\n",
    "random_image = Image.open(random_image_path)\n",
    "imshow(process_image(random_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa56993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class from an image file:\n",
    "\n",
    "def predict(pil_image, model, top_k_probabilities = 5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    \n",
    "    # Use GPU if it's available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #print(device)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    np_image = process_image(pil_image)\n",
    "    tensor_image = torch.from_numpy(np_image)\n",
    "    \n",
    "    inputs = Variable(tensor_image)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(tensor_image.float().cuda())           \n",
    "        \n",
    "    inputs = inputs.unsqueeze(dim = 0)\n",
    "    log_probabilities = model.forward(inputs)\n",
    "    probabilities = torch.exp(log_probabilities)    \n",
    "\n",
    "    top_probabilities, top_classes = probabilities.topk(top_k_probabilities, dim = 1)\n",
    "    #print(top_probabilities)\n",
    "    #print(top_classes)\n",
    "    \n",
    "    class_to_idx_inverted = {model.class_to_idx[c]: c for c in model.class_to_idx}\n",
    "    top_mapped_classes = list()\n",
    "    \n",
    "    for label in top_classes.cpu().detach().numpy()[0]:\n",
    "        top_mapped_classes.append(class_to_idx_inverted[label])\n",
    "    \n",
    "    return top_probabilities.cpu().detach().numpy()[0], top_mapped_classes\n",
    "\n",
    "# with open(classes_json, 'r') as f:\n",
    "#     category_label_to_name = json.load(f)\n",
    "\n",
    "top_probabilities, top_classes = predict(random_image, model, top_k_probabilities = 5)\n",
    "\n",
    "for c in top_classes:\n",
    "    if int(c) < 10: # overcome the '0' padding in the filename\n",
    "        c = str(int(c))\n",
    "    print(classes[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image along with the top 5 classes\n",
    "\n",
    "max_index = np.argmax(top_probabilities)\n",
    "max_probability = top_probabilities[max_index]\n",
    "label = top_classes[max_index]\n",
    "\n",
    "if int(label) < 10: # overcome the '0' padding in the filename\n",
    "    label = str(int(label))\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = plt.subplot2grid((15,9), (0,0), colspan = 9, rowspan = 9)\n",
    "ax2 = plt.subplot2grid((15,9), (9,2), colspan = 5, rowspan = 5)\n",
    "\n",
    "ax1.axis('off')\n",
    "ax1.set_title(classes[label])\n",
    "ax1.imshow(random_image)\n",
    "\n",
    "labels = []\n",
    "for c in top_classes:\n",
    "    if int(c) < 10: # overcome the '0' padding in the filename\n",
    "        c = str(int(c))\n",
    "    labels.append(classes[c])\n",
    "\n",
    "y_pos = np.arange(5)\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(labels)\n",
    "ax2.set_xlabel('Probability')\n",
    "ax2.invert_yaxis()\n",
    "ax2.barh(y_pos, top_probabilities, xerr = 0, align = 'center', color = 'blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity check - display an image along with the top 5 classes\n",
    "\n",
    "# test_case = 10\n",
    "\n",
    "# for i in range(test_case):\n",
    "#     flower_class = str(np.random.randint(1, len(classes) + 1))\n",
    "#     print(flower_class)\n",
    "#     image_path = test_data + '/' + flower_class + '/' + os.listdir(test_dir + '/' + flower_class + '/')[0]\n",
    "#     print(image_path)\n",
    "\n",
    "#     pil_image = Image.open(image_path)\n",
    "#     plt.imshow(pil_image)\n",
    "\n",
    "#     top_probabilities, top_classes = predict(pil_image, model, top_k_probabilities = 5)\n",
    "#     max_index = np.argmax(top_probabilities)\n",
    "#     max_probability = top_probabilities[max_index]\n",
    "#     label = top_classes[max_index]\n",
    "\n",
    "#     fig = plt.figure(figsize=(6,6))\n",
    "#     ax1 = plt.subplot2grid((15,9), (0,0), colspan = 9, rowspan = 9)\n",
    "#     ax2 = plt.subplot2grid((15,9), (9,2), colspan = 5, rowspan = 5)\n",
    "\n",
    "#     ax1.axis('off')\n",
    "#     ax1.set_title(classes[flower_class]) #Real class\n",
    "#     ax1.imshow(pil_image)\n",
    "\n",
    "#     labels = []\n",
    "#     for c in top_classes:\n",
    "#         if int(c) < 10: # overcome the '0' padding in the filename\n",
    "#             c = str(int(c))\n",
    "#         labels.append(classes[c])\n",
    "\n",
    "#     y_pos = np.arange(5)\n",
    "#     ax2.set_yticks(y_pos)\n",
    "#     ax2.set_yticklabels(labels)\n",
    "#     ax2.set_xlabel('Probability')\n",
    "#     ax2.invert_yaxis()\n",
    "#     ax2.barh(y_pos, top_probabilities, xerr = 0, align = 'center', color = 'blue')\n",
    "\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
