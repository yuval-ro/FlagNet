{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9a4dfb1-584f-4c6e-b39a-c32fc1863ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module imports\n",
    "\n",
    "# vanilla modules:\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from time import strptime\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "# external modules:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import PIL.Image as Image\n",
    "import git\n",
    "import splitfolders\n",
    "\n",
    "# Prerequisites:\n",
    "# NumPy             https://numpy.org/doc/stable/\n",
    "# Matplotlib        https://matplotlib.org/stable/index.html\n",
    "# PyTorch           https://pytorch.org/docs/stable/index.html\n",
    "# Torchvision       https://pytorch.org/docs/stable/index.html\n",
    "# PIL               https://pillow.readthedocs.io/en/stable/\n",
    "# GitPython         https://gitpython.readthedocs.io/en/stable/\n",
    "# split-folders:    https://pypi.org/project/split-folders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e2aa396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions definitions\n",
    "\n",
    "\n",
    "class Style:\n",
    "\tDEFAULT =\t'\\033[0m'\n",
    "\tBOLD = \t\t'\\033[1m'\n",
    "\tBLACK = \t'\\033[30m'\n",
    "\tRED =\t\t'\\033[31m'\n",
    "\tGREEN =\t\t'\\033[32m'\n",
    "\tYELLOW =\t'\\033[33m'\n",
    "\tBLUE =\t\t'\\033[34m'\n",
    "\tMAGENTA =\t'\\033[35m'\n",
    "\tCYAN =\t\t'\\033[36m'\n",
    "\tWHITE =\t\t'\\033[37m'\n",
    "\n",
    "\n",
    "def color(color, bold=False):\n",
    "\tif color in ['d', 'default']:\n",
    "\t\treturn (Style.DEFAULT if not bold else Style.BOLD+Style.DEFAULT)\n",
    "\tif color in ['r', 'red']:\n",
    "\t\treturn (Style.RED if not bold else Style.BOLD+Style.RED)\n",
    "\tif color in ['g', 'green']:\n",
    "\t\treturn (Style.GREEN if not bold else Style.BOLD+Style.GREEN)\n",
    "\tif color in ['y', 'yellow']:\n",
    "\t\treturn (Style.YELLOW if not bold else Style.BOLD+Style.YELLOW)\n",
    "\telse:\n",
    "\t\traise SystemExit(f'passed unfamilier parameters passed to color()')\n",
    "\n",
    "def println(strings, width=20, header=False):\n",
    "\tif header: # header lines\n",
    "\t\tfor idx, s in enumerate(strings):\n",
    "\t\t\tif isinstance(s, tuple):\n",
    "\t\t\t\tSystemExit(f'passed a tuple while header=True')\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(Style.BOLD+('{0:<{1}}').format(str(s), width), end='')\n",
    "\t\t\tif idx == (len(strings) - 1):\n",
    "\t\t\t\tprint(color('d'))\n",
    "\telse:\n",
    "\t\tfor idx, s in enumerate(strings):\n",
    "\t\t\tif isinstance(s, tuple): # color or bold applied\n",
    "\t\t\t\tprint((color(s[1])+('{0:<{1}.3f}').format((s[0]), width) if isinstance(s[0], float) else color(s[1])+('{0:<{1}}').format(str(s[0]), width)), end='')\n",
    "\n",
    "\t\t\telse: # no color or bold needed\n",
    "\t\t\t\tprint(color('d')+(('{0:<{1}.3f}').format(s, width) if isinstance(s, float) else color('d')+('{0:<{1}}').format(str(s), width)), end='')\n",
    "\n",
    "\t\t\tif idx == (len(strings) - 1):\n",
    "\t\t\t\tprint(color('d'))\n",
    "\n",
    "def seconds_to_time(seconds, hrs=False):\n",
    "    if hrs:\n",
    "        return '%02d:%02d:%02d'%((seconds // 3600), (seconds // 60), (round(seconds % 60)))\n",
    "    else:\n",
    "        return '%02d:%02d'%((seconds // 60), (round(seconds % 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e5fa87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data repository cloning\n",
    "\n",
    "\n",
    "token = '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'\n",
    "username = 'yuval-ro'\n",
    "remote_repo = 'FlagNet_data'\n",
    "local_repo = remote_repo\n",
    "remote_repo_url = f'https://{token}@github.com/{username}/{remote_repo}.git'\n",
    "\n",
    "try:\n",
    "    git.Repo.clone_from(remote_repo_url, local_repo)\n",
    "except:\n",
    "    git.rmtree(local_repo)\n",
    "    git.Repo.clone_from(remote_repo_url, local_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84a41e81-7b80-4b39-be79-c40461103f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mparsing the json file\u001b[0m\n",
      "[{'1': 'australia', '2': 'brazil', '3': 'canada', '4': 'china', '5': 'france', '6': 'germany', '7': 'india', '8': 'israel', '9': 'italy', '10': 'japan', '11': 'russia', '12': 'spain', '13': 'sweden', '14': 'uk', '15': 'usa'}, {'images_per_class': 30}]\n",
      "\u001b[1mtotal classes       \u001b[1m                    \u001b[1mimages per class    \u001b[0m\n",
      "\u001b[0m\u001b[0m15                  \u001b[0m\u001b[0m                    \u001b[0m\u001b[0m30                  \u001b[0m\n",
      "\n",
      "\u001b[1mid                  \u001b[1mparsed class        \u001b[1mimages found        \u001b[0m\n",
      "\u001b[0m\u001b[0m1                   \u001b[0m\u001b[0mAustralia           \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m2                   \u001b[0m\u001b[0mBrazil              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m3                   \u001b[0m\u001b[0mCanada              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m4                   \u001b[0m\u001b[0mChina               \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m5                   \u001b[0m\u001b[0mFrance              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m6                   \u001b[0m\u001b[0mGermany             \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m7                   \u001b[0m\u001b[0mIndia               \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m8                   \u001b[0m\u001b[0mIsrael              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m9                   \u001b[0m\u001b[0mItaly               \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m10                  \u001b[0m\u001b[0mJapan               \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m11                  \u001b[0m\u001b[0mRussia              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m12                  \u001b[0m\u001b[0mSpain               \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m13                  \u001b[0m\u001b[0mSweden              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m14                  \u001b[0m\u001b[0mUK                  \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m\u001b[0m15                  \u001b[0m\u001b[0mUSA                 \u001b[32m30                  \u001b[0m\n",
      "\n",
      "\u001b[1m                    \u001b[1m                    \u001b[1mtotal images        \u001b[0m\n",
      "\u001b[0m\u001b[0m                    \u001b[0m\u001b[0m                    \u001b[32m450                 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Dataset sanity check\n",
    "\n",
    "println([('parsing the json file', 'y')])\n",
    "\n",
    "dataset_dir = local_repo + '\\\\dataset'\n",
    "classes_json = local_repo + '\\\\classes.json'\n",
    "images_found_per_class = []\n",
    "bad_dirs = []\n",
    "json_ne_dirs = False\n",
    "\n",
    "# Locates and parses the \"classes.json\" file for label mapping:\n",
    "with open(classes_json, 'r') as f:\n",
    "    json_file = json.load(f)\n",
    "    classes = OrderedDict(json_file[0])\n",
    "    images_per_class = json_file[1]['images_per_class']\n",
    "    dir_names = [dataset_dir + '\\\\%.2d' % i for i in range(1, len(classes) + 1)]\n",
    "\n",
    "# Display json metadata:\n",
    "println(['total classes', '', 'images per class'], header=True)\n",
    "println([len(classes), '', images_per_class])\n",
    "print()\n",
    "\n",
    "\n",
    "# Checks the number of classes defined in the json equal to number of classes subdirs:\n",
    "if len(os.listdir(dataset_dir)) != len(classes):\n",
    "    json_ne_dirs=True\n",
    "\n",
    "# Checks the number of images of each class subdir:\n",
    "for dir_tuple in os.walk(dataset_dir):\n",
    "    if dir_tuple[0] in dir_names: # skips junk directories\n",
    "        images_in_dir = len(dir_tuple[2])\n",
    "        images_found_per_class.append(images_in_dir)\n",
    "        if images_in_dir != images_per_class:\n",
    "            bad_dirs.append(dir_tuple[0])\n",
    "\n",
    "# Displays the metadata nicely:\n",
    "println(['id', 'parsed class', 'images found'], header=True)\n",
    "\n",
    "for i, (ID, Class) in enumerate(classes.items()):\n",
    "    println([ID,\n",
    "            Class.upper() if Class in ['uk','usa'] else Class.capitalize(),\n",
    "            ((images_found_per_class[i], 'g') if images_found_per_class[i] == images_per_class else (images_found_per_class[i], 'r'))])\n",
    "\n",
    "print()\n",
    "println(['','','total images'], header=True)\n",
    "\n",
    "println(['','',((sum(images_found_per_class), 'g') if sum(images_found_per_class) == (len(classes) * images_per_class) else (sum(images_found_per_class), 'r'))])\n",
    "\n",
    "# Throws exceptions if needed:\n",
    "if json_ne_dirs:\n",
    "    raise SystemExit(f'number of classes according to the json file ({len(classes)}) does not correlate with total dirs ({len(os.listdir(dataset_dir))}) in \\\"{dataset_dir}\\\"')\n",
    "elif bad_dirs != []:\n",
    "    raise SystemExit(f'image count in the following directories is incorrect: {bad_dirs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36ec9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Image pre-processing hyperparameters:\n",
    "degrees_rotation = 30\n",
    "size_crop = 224\n",
    "size_resize = 256\n",
    "normalize_mean = [0.485, 0.456, 0.406]\n",
    "normalize_std = [0.229, 0.224, 0.225]\n",
    "batch_size = 32\n",
    "\n",
    "# Define a transform for each set:\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(degrees_rotation),\n",
    "                                       transforms.RandomResizedCrop(size_crop),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(normalize_mean, normalize_std)\n",
    "                                      ])\n",
    "\n",
    "valid_transforms = transforms.Compose([transforms.Resize(size_resize), \n",
    "                                       transforms.CenterCrop(size_crop),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(normalize_mean, normalize_std)\n",
    "                                      ])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(size_resize),\n",
    "                                      transforms.CenterCrop(size_crop),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(normalize_mean, normalize_std)\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7a462ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 450 files [00:01, 276.21 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Dataset parsing and creation of DataLoaders\n",
    "\n",
    "\n",
    "# Randomly split the dataset into three folders with the following ratio of images:\n",
    "ratio = (.8, .1, .1) # Defined ratio of set size\n",
    "sets_path = 'sets'\n",
    "train_set_path = sets_path + '\\\\train'\n",
    "valid_set_path = sets_path + '\\\\val'\n",
    "test_set_path = sets_path + '\\\\test'\n",
    "\n",
    "if os.path.exists(sets_path):\n",
    "    shutil.rmtree(sets_path)\n",
    "\n",
    "splitfolders.ratio(\n",
    "    dataset_dir,\n",
    "    output=sets_path,\n",
    "    seed=1337,\n",
    "    ratio=ratio,\n",
    "    group_prefix=None,\n",
    "    move=False)\n",
    "\n",
    "# Instanciate each set:\n",
    "train_data = datasets.ImageFolder(train_set_path, transform = train_transforms)\n",
    "valid_data = datasets.ImageFolder(valid_set_path, transform = valid_transforms)\n",
    "test_data = datasets.ImageFolder(test_set_path, transform = test_transforms)\n",
    "\n",
    "# Create the DataLoader for each set:\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ffdceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation and classifier definition\n",
    "\n",
    "\n",
    "# Instanciate the model (VGG16) and define a classifier for it:\n",
    "model = models.vgg16(pretrained = False)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "dropout_probability = 0.5\n",
    "in_features = 25088\n",
    "out_features = 1024\n",
    "\n",
    "classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(in_features, out_features)),\n",
    "                                        ('drop', nn.Dropout(p = dropout_probability)),\n",
    "                                        ('relu', nn.ReLU()),\n",
    "                                        ('fc2', nn.Linear(out_features, len(classes))),\n",
    "                                        ('output', nn.LogSoftmax(dim = 1))\n",
    "                                       ]))\n",
    "\n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e6f8651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Loading a checkpoint\n",
    "# Template for loading a checkpoint:\n",
    "def load_checkpoint(file_path):\n",
    "    checkpoint = torch.load(file_path)\n",
    "    learning_rate = checkpoint['learning_rate']\n",
    "    model = getattr(torchvision.models, checkpoint['network'])(pretrained=True)\n",
    "    model.classifier = checkpoint['classifier']\n",
    "    model.epochs = checkpoint['epochs']\n",
    "    model.optimizer = checkpoint['optimizer']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Sort all checkpoints by their filename (date created, ascending order):\n",
    "timestamp_format = '%H%M%S_%d%m%y'\n",
    "checkpoints = []\n",
    "for i in os.listdir():\n",
    "    if i.endswith('.pth'):\n",
    "        filename, ext = os.path.splitext(i)\n",
    "        filename_date_tuple = (time.strptime(filename, timestamp_format), i)\n",
    "        checkpoints.append(filename_date_tuple)\n",
    "\n",
    "\n",
    "# If no .pth file were found raise an exception:\n",
    "if checkpoints == []:\n",
    "    raise SystemExit(f'no .pth were found whilst trying to load a checkpoint')\n",
    "\n",
    "\n",
    "# Get the latest checkpoint and load it onto the model instance\n",
    "latest_checkpoint = (sorted(checkpoints, key=lambda x: x[0])[-1])[1]\n",
    "model = load_checkpoint(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05c4a97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mdevice              \u001b[1mepochs              \u001b[1mcriterion           \u001b[1moptimizer           \u001b[1mlearning rate       \u001b[1mvalidation          \u001b[0m\n",
      "\u001b[32mGPU                 \u001b[0m\u001b[0m10                  \u001b[0m\u001b[0mNLL                 \u001b[0m\u001b[0mAdam                \u001b[0m0.001               \u001b[0m\u001b[0myes                 \u001b[0m\n",
      "\n",
      "\u001b[33mtraining on GPU started\u001b[0m\n",
      "\u001b[1mepoch               \u001b[1mtime                \u001b[1mtrain loss          \u001b[1mvalid loss          \u001b[1maccuracy            \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# Training the model while collecting metadata:\n",
    "\n",
    "\n",
    "# Hyperparameters:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_name = ('GPU' if device == torch.device('cuda') else 'CPU')\n",
    "epochs = 10\n",
    "criterion = nn.NLLLoss()\n",
    "criterion_name = 'NLL'\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr = learning_rate) # Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer_name = 'Adam'\n",
    "validation_step = True\n",
    "\n",
    "\n",
    "model.to(device) # cast the model to the available device\n",
    "epochs_metadata = [] # save metadata about each epoch for monitoring\n",
    "\n",
    "\n",
    "\n",
    "# Display hyperparameters:\n",
    "println(['device', 'epochs', 'criterion', 'optimizer', 'learning rate', 'validation'], header=True)\n",
    "println([(device_name, ('g' if device_name == 'GPU' else 'r')),\n",
    "        epochs,\n",
    "        criterion_name,\n",
    "        optimizer_name,\n",
    "        learning_rate,\n",
    "        ('yes' if validation_step else 'no')])\n",
    "print()\n",
    "\n",
    "\n",
    "println([(f'training on {device_name} started', 'y')])\n",
    "println(['epoch', 'time', 'train loss', 'valid loss', 'accuracy'], header=True)\n",
    "start_training_time = time.time()\n",
    "# Training loop:\n",
    "for epoch in range(epochs):\n",
    "    start_epoch_time = time.time()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Batch (train set) loop:\n",
    "    for inputs, labels in train_loader:     \n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        log_probabilities = model.forward(inputs)\n",
    "        loss = criterion(log_probabilities, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = train_loss + loss.item()\n",
    "    \n",
    "    # Batch (valid set) loop:\n",
    "    if validation_step:\n",
    "        valid_loss = 0\n",
    "        valid_accuracy = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                log_probabilities = model.forward(inputs)\n",
    "                loss = criterion(log_probabilities, labels)\n",
    "                valid_loss = valid_loss + loss.item()\n",
    "                # Calculate accuracy\n",
    "                probabilities = torch.exp(log_probabilities)\n",
    "                top_probability, top_class = probabilities.topk(1, dim = 1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                valid_accuracy = valid_accuracy + torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        model.train()\n",
    "\n",
    "    end_epoch_time = time.time()\n",
    "    epochs_metadata.append([(epoch + 1),                            # epoch index\n",
    "                            (end_epoch_time - start_epoch_time),    # epoch time in seconds\n",
    "                            (train_loss / len(train_loader)),       # epoch training loss\n",
    "                            (valid_loss / len(valid_loader)),       # epoch validation loss\n",
    "                            (valid_accuracy / len(valid_loader))    # epoch accuracy\n",
    "    ])\n",
    "    println([epochs_metadata[epoch][0],\n",
    "             seconds_to_time(epochs_metadata[epoch][1]),\n",
    "             epochs_metadata[epoch][2],\n",
    "             epochs_metadata[epoch][3],\n",
    "             epochs_metadata[epoch][4]])\n",
    "end_training_time = time.time()\n",
    "total_training_time = (start_training_time - end_training_time)\n",
    "\n",
    "\n",
    "# Display the collected metadata:\n",
    "print()\n",
    "println([(f'training on {device_name} finished', 'y')])\n",
    "println(['epochs', 'total time', 'mean train loss', 'mean valid loss', 'mean accuracy'], header=True)\n",
    "println([len(epochs_metadata),\n",
    "        seconds_to_time(np.sum([i[1] for i in epochs_metadata])),\n",
    "        np.mean([i[2] for i in epochs_metadata]),\n",
    "        np.mean([i[3] for i in epochs_metadata]),\n",
    "        np.mean([i[4] for i in epochs_metadata]),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cbc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model testing\n",
    "\n",
    "\n",
    "# test_loss = 0\n",
    "# test_accuracy = 0\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "# print('Testing started')\n",
    "# start_time = time.time()\n",
    "\n",
    "# for inputs, labels in test_loader:\n",
    "#     inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#     log_probabilities = model.forward(inputs)\n",
    "#     loss = criterion(log_probabilities, labels)\n",
    "\n",
    "#     test_loss = test_loss + loss.item()\n",
    "\n",
    "#     # Calculate accuracy\n",
    "#     probabilities = torch.exp(log_probabilities)\n",
    "#     top_probability, top_class = probabilities.topk(1, dim = 1)\n",
    "\n",
    "#     equals = top_class == labels.view(*top_class.shape)\n",
    "\n",
    "#     test_accuracy = test_accuracy + torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "# end_time = time.time()\n",
    "# print('Testing ended')\n",
    "# validation_time = datetimeend_time - start_time\n",
    "\n",
    "# print('Validation time: {:.0f}m {:.0f}s'.format(validation_time / 60, validation_time % 60))\n",
    "\n",
    "# println(['type', 'loss', 'accuracy'], bold=True)\n",
    "# println([\n",
    "#     'validation',\n",
    "#     '%.3f'%(test_loss / len(test_loader)),\n",
    "#     '%.3f'%(test_accuracy / len(test_loader))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "\n",
    "print('{0:<20}'.format('text'),'{0:<20}'.format('text'))\n",
    "print(Style.GREEN+'{0:<20}'.format('text'),Style.WHITE+'{0:<20}'.format('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the checkpoint:\n",
    "# timestamp_format = '%H%M%S_%d%m%y'\n",
    "# timestamp = datetime.datetime.now().strftime(timestamp_format)\n",
    "# checkpoint_name = f'{timestamp}.pth'\n",
    "\n",
    "# model.class_to_idx = train_data.class_to_idx\n",
    "\n",
    "# checkpoint = {'network': 'vgg16',\n",
    "#               'input_size': in_features,\n",
    "#               'output_size': len(classes),\n",
    "#               'learning_rate': learning_rate,       \n",
    "#               'batch_size': batch_size,\n",
    "#               'classifier' : classifier,\n",
    "#               'epochs': epochs,\n",
    "#               'optimizer': optimizer.state_dict(),\n",
    "#               'state_dict': model.state_dict(),\n",
    "#               'class_to_idx': model.class_to_idx}\n",
    "\n",
    "# torch.save(checkpoint, checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_image(pil_image):\n",
    "#     ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "#         returns an Numpy array\n",
    "#     '''\n",
    "    \n",
    "#     img_loader = transforms.Compose([transforms.Resize(size_resize),\n",
    "#                                      transforms.CenterCrop(size_crop), \n",
    "#                                      transforms.ToTensor()])\n",
    "    \n",
    "#     #pil_image = Image.open(image)\n",
    "#     pil_image = img_loader(pil_image).float()\n",
    "    \n",
    "#     np_image = np.array(pil_image)    \n",
    "    \n",
    "#     mean = np.array(normalize_mean)\n",
    "#     std = np.array(normalize_std)\n",
    "#     np_image = (np.transpose(np_image, (1, 2, 0)) - mean) / std    \n",
    "#     np_image = np.transpose(np_image, (2, 0, 1))\n",
    "            \n",
    "#     return np_image\n",
    "\n",
    "# def imshow(np_image, ax = None, title = None):\n",
    "#     if ax is None:\n",
    "#         fig, ax = plt.subplots()\n",
    "    \n",
    "#     # PyTorch tensors assume the color channel is the first dimension\n",
    "#     # but matplotlib assumes is the third dimension\n",
    "#     np_image = np.transpose(np_image, (1, 2, 0))\n",
    "    \n",
    "#     # Undo preprocessing\n",
    "#     mean = np.array(normalize_mean)\n",
    "#     std = np.array(normalize_std)\n",
    "#     np_image = std * np_image + mean\n",
    "    \n",
    "#     # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "#     np_image = np.clip(np_image, 0, 1)\n",
    "    \n",
    "#     ax.imshow(np_image)\n",
    "    \n",
    "#     return ax\n",
    "\n",
    "\n",
    "# images_paths = result = [os.path.join(dp, f) for dp, dn, filenames in os.walk(sets_path) for f in filenames if os.path.splitext(f)[1] == '.jpg']\n",
    "# random.seed(10)\n",
    "# random_image_path = random.choice(images_paths)\n",
    "# random_image = Image.open(random_image_path)\n",
    "# imshow(process_image(random_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa56993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the class from an image file:\n",
    "\n",
    "# def predict(pil_image, model, top_k_probabilities = 5):\n",
    "#     ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "#     '''\n",
    "    \n",
    "#     # Use GPU if it's available\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     #print(device)\n",
    "\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     np_image = process_image(pil_image)\n",
    "#     tensor_image = torch.from_numpy(np_image)\n",
    "    \n",
    "#     inputs = Variable(tensor_image)\n",
    "    \n",
    "#     if torch.cuda.is_available():\n",
    "#         inputs = Variable(tensor_image.float().cuda())           \n",
    "        \n",
    "#     inputs = inputs.unsqueeze(dim = 0)\n",
    "#     log_probabilities = model.forward(inputs)\n",
    "#     probabilities = torch.exp(log_probabilities)    \n",
    "\n",
    "#     top_probabilities, top_classes = probabilities.topk(top_k_probabilities, dim = 1)\n",
    "#     #print(top_probabilities)\n",
    "#     #print(top_classes)\n",
    "    \n",
    "#     class_to_idx_inverted = {model.class_to_idx[c]: c for c in model.class_to_idx}\n",
    "#     top_mapped_classes = list()\n",
    "    \n",
    "#     for label in top_classes.cpu().detach().numpy()[0]:\n",
    "#         top_mapped_classes.append(class_to_idx_inverted[label])\n",
    "    \n",
    "#     return top_probabilities.cpu().detach().numpy()[0], top_mapped_classes\n",
    "\n",
    "# # with open(classes_json, 'r') as f:\n",
    "# #     category_label_to_name = json.load(f)\n",
    "\n",
    "# top_probabilities, top_classes = predict(random_image, model, top_k_probabilities = 5)\n",
    "\n",
    "# for c in top_classes:\n",
    "#     if int(c) < 10: # overcome the '0' padding in the filename\n",
    "#         c = str(int(c))\n",
    "#     print(classes[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display an image along with the top 5 classes\n",
    "\n",
    "# max_index = np.argmax(top_probabilities)\n",
    "# max_probability = top_probabilities[max_index]\n",
    "# label = top_classes[max_index]\n",
    "\n",
    "# if int(label) < 10: # overcome the '0' padding in the filename\n",
    "#     label = str(int(label))\n",
    "\n",
    "# fig = plt.figure(figsize=(6,6))\n",
    "# ax1 = plt.subplot2grid((15,9), (0,0), colspan = 9, rowspan = 9)\n",
    "# ax2 = plt.subplot2grid((15,9), (9,2), colspan = 5, rowspan = 5)\n",
    "\n",
    "# ax1.axis('off')\n",
    "# ax1.set_title(classes[label])\n",
    "# ax1.imshow(random_image)\n",
    "\n",
    "# labels = []\n",
    "# for c in top_classes:\n",
    "#     if int(c) < 10: # overcome the '0' padding in the filename\n",
    "#         c = str(int(c))\n",
    "#     labels.append(classes[c])\n",
    "\n",
    "# y_pos = np.arange(5)\n",
    "# ax2.set_yticks(y_pos)\n",
    "# ax2.set_yticklabels(labels)\n",
    "# ax2.set_xlabel('Probability')\n",
    "# ax2.invert_yaxis()\n",
    "# ax2.barh(y_pos, top_probabilities, xerr = 0, align = 'center', color = 'blue')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity check - display an image along with the top 5 classes\n",
    "\n",
    "# test_case = 10\n",
    "\n",
    "# for i in range(test_case):\n",
    "#     flower_class = str(np.random.randint(1, len(classes) + 1))\n",
    "#     print(flower_class)\n",
    "#     image_path = test_data + '/' + flower_class + '/' + os.listdir(test_dir + '/' + flower_class + '/')[0]\n",
    "#     print(image_path)\n",
    "\n",
    "#     pil_image = Image.open(image_path)\n",
    "#     plt.imshow(pil_image)\n",
    "\n",
    "#     top_probabilities, top_classes = predict(pil_image, model, top_k_probabilities = 5)\n",
    "#     max_index = np.argmax(top_probabilities)\n",
    "#     max_probability = top_probabilities[max_index]\n",
    "#     label = top_classes[max_index]\n",
    "\n",
    "#     fig = plt.figure(figsize=(6,6))\n",
    "#     ax1 = plt.subplot2grid((15,9), (0,0), colspan = 9, rowspan = 9)\n",
    "#     ax2 = plt.subplot2grid((15,9), (9,2), colspan = 5, rowspan = 5)\n",
    "\n",
    "#     ax1.axis('off')\n",
    "#     ax1.set_title(classes[flower_class]) #Real class\n",
    "#     ax1.imshow(pil_image)\n",
    "\n",
    "#     labels = []\n",
    "#     for c in top_classes:\n",
    "#         if int(c) < 10: # overcome the '0' padding in the filename\n",
    "#             c = str(int(c))\n",
    "#         labels.append(classes[c])\n",
    "\n",
    "#     y_pos = np.arange(5)\n",
    "#     ax2.set_yticks(y_pos)\n",
    "#     ax2.set_yticklabels(labels)\n",
    "#     ax2.set_xlabel('Probability')\n",
    "#     ax2.invert_yaxis()\n",
    "#     ax2.barh(y_pos, top_probabilities, xerr = 0, align = 'center', color = 'blue')\n",
    "\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
