{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4dfb1-584f-4c6e-b39a-c32fc1863ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing Modules'''\n",
    "\n",
    "\n",
    "# vanilla:\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from time import strptime\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import sys\n",
    "# external:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import PIL.Image as Image\n",
    "import git\n",
    "import splitfolders\n",
    "from dotenv import load_dotenv\n",
    "# custom:\n",
    "from routines import *\n",
    "from displays import *\n",
    "import myTransforms\n",
    "import consts\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6032a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Parsing the .env File'''\n",
    "\n",
    "\n",
    "# Loading sensitive info from the dotenv file.\n",
    "# It is needed in order to clone the data repo:\n",
    "if not load_dotenv(consts.dotenv_path):\n",
    "    e_msg = 'cannot find the required .env file'\n",
    "    raise SystemExit(e_msg)\n",
    "gh_token = os.getenv('GH_TOKEN')\n",
    "gh_username = os.getenv('GH_USERNAME')\n",
    "repo_name = os.getenv('REMOTE_REPO_NAME')\n",
    "repo_url = f'https://{gh_token}@github.com/{gh_username}/{repo_name}.git'\n",
    "repo_dir_path = f'./{repo_name}'\n",
    "dataset_dir_path = repo_dir_path + '/dataset'\n",
    "classes_file_path = repo_dir_path + '/classes.json'\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5fa87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cloning the Remote Data-Repository'''\n",
    "\n",
    "url_issue = False\n",
    "# Checks if a leftover repo exists, overwrite it if so:\n",
    "if os.path.exists(repo_dir_path):\n",
    "    git.rmtree(repo_dir_path)\n",
    "# Clones the repo, and raises an exception if the remote URL is corrupted:\n",
    "try:\n",
    "    git.Repo.clone_from(repo_url, repo_name)\n",
    "except Exception as e:\n",
    "    url_issue = True\n",
    "    pass\n",
    "if url_issue:\n",
    "    e_msg = 'there is an issue with the remote repo URL'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a22b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Parsing the JSON File from the Data Repository'''\n",
    "\n",
    "\n",
    "json_not_found = False\n",
    "try:\n",
    "    with open(classes_file_path, 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "        classes = OrderedDict(json_file[0])\n",
    "        images_per_class = json_file[1]['images_per_class']\n",
    "        # Creates a list of all subdir names (strings) within dataset dir:\n",
    "        dir_names = [dataset_dir_path + '\\\\%.2d' % i for i in range(1, len(classes) + 1)]\n",
    "    # Displays the JSON file metadata:\n",
    "    println(['total classes', 'images per class'], header=True)\n",
    "    println([len(classes), images_per_class])\n",
    "except FileNotFoundError as e:\n",
    "    json_not_found=True\n",
    "if json_not_found:\n",
    "    e_msg=f'cannot locate the \\'classes.json\\' file in \"{repo_name}\".'\\\n",
    "        + f'\\nre-run the \\'Cloning the Remote Data-Repositoryg\\' cell and try again.'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a41e81-7b80-4b39-be79-c40461103f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Validating the Dataset Directory'''\n",
    "\n",
    "\n",
    "println([('performing a valdiation of the cloned data repo according its JSON file,\\n\\\n",
    "before any further training can take place...', 'y')])\n",
    "\n",
    "files_per_class = []\n",
    "bad_dirs = []\n",
    "json_ne_dirs = False\n",
    "\n",
    "# Validates the number of classes defined in the JSON equals to number of classes subdirs:\n",
    "if len(os.listdir(dataset_dir_path)) != len(classes):\n",
    "    json_ne_dirs=True\n",
    "\n",
    "# Validates that the number of images in each class subdir equals to the one defined in the JSON:\n",
    "for dir_tuple in os.walk(dataset_dir_path):\n",
    "    if dir_tuple[0] in dir_names: # skips junk directories\n",
    "        images_in_dir = len(dir_tuple[2])\n",
    "        files_per_class.append(images_in_dir)\n",
    "        if images_in_dir != images_per_class:\n",
    "            bad_dirs.append(dir_tuple[0])\n",
    "\n",
    "# Raise exceptions if needed:\n",
    "if json_ne_dirs:\n",
    "    e_msg=f'number of classes according to the JSON file ({len(classes)})'\\\n",
    "        + f' does not correlate with total dirs ({len(os.listdir(dataset_dir_path))})'\\\n",
    "        + f' in \\\"{dataset_dir_path}\\\".'\\\n",
    "        + f'\\nre-run \\'Data Repository Cloning\\' cell then re-run this cell.'\n",
    "    raise SystemExit(e_msg)\n",
    "elif bad_dirs != []:\n",
    "    e_msg=f'image count in the following directories is incorrect: {bad_dirs}'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "# Displaying...\n",
    "# If the number of files found in a class subdir does not strictly equal\n",
    "#  to the defined number (from the JSON file), the number will be highlighted\n",
    "#  with red color; elsewise, in green.\n",
    "println(['id', 'parsed class', 'images found'], header=True)\n",
    "for i, (ID, Class) in enumerate(classes.items()):\n",
    "    println([ID,\n",
    "            Class.upper() if Class in ['uk','usa'] else Class.capitalize(),\n",
    "            (files_per_class[i], ('g' if files_per_class[i] == images_per_class else 'r'))])\n",
    "println(['','','total images'], header=True)\n",
    "println(['','',(sum(files_per_class), ('g' if (sum(files_per_class) == (len(classes) * images_per_class)) else 'r'))])\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a462ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Splitting the Dataset'''\n",
    "\n",
    "\n",
    "println([('creating a new \\'sets\\' dir, with three subdirs of images: \\\n",
    "\\'train\\', \\'valid\\', \\'test\\'...', 'y')])\n",
    "\n",
    "# Deleting a leftover 'sets' directory if such exists:\n",
    "\n",
    "if os.path.exists(consts.sets_path):\n",
    "    shutil.rmtree(consts.sets_path)\n",
    "\n",
    "# Randomly splitting the dataset into 'test', 'valid', 'test' image directories:\n",
    "splitfolder_issue = False\n",
    "try:\n",
    "    splitfolders.ratio(\n",
    "        dataset_dir_path,\n",
    "        output=consts.sets_path,\n",
    "        seed=1337,\n",
    "        ratio=(.8, .1, .1),\n",
    "        group_prefix=None,\n",
    "        move=False)\n",
    "except:\n",
    "    splitfolder_issue = True\n",
    "if splitfolder_issue:\n",
    "    e_msg = 'exception raised within a splitfolders.ratio() call'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83746019",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creating DataLoaders'''\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "print(f'batch size is {batch_size}')\n",
    "\n",
    "# Instanciating each set:\n",
    "train_data = datasets.ImageFolder(consts.trainset_path, transform=myTransforms.train_transforms)\n",
    "valid_data = datasets.ImageFolder(consts.validset_path, transform=myTransforms.valid_transforms)\n",
    "test_data = datasets.ImageFolder(consts.testset_path, transform=myTransforms.test_transforms)\n",
    "\n",
    "# Creating a DataLoader for each set:\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instaciating a Model and a Classifier'''\n",
    "\n",
    "\n",
    "pretrained = True\n",
    "weights=('DEFAULT' if pretrained else None)\n",
    "model = models.vgg16(weights=weights)\n",
    "model_name = 'VGG16'\n",
    "for param in model.parameters():\n",
    "    # Freeze the MODEL parameters so we don't backprop through them! Only through the classifier.\n",
    "    param.requires_grad = False\n",
    "dropout_probability = .5\n",
    "in_features = 25088\n",
    "out_features = 1024\n",
    "od = OrderedDict([('fc1', nn.Linear(in_features, out_features)),\n",
    "                ('drop', nn.Dropout(p=dropout_probability)),\n",
    "                ('relu', nn.ReLU()),\n",
    "                ('fc2', nn.Linear(out_features, len(classes))),\n",
    "                ('output', nn.LogSoftmax(dim=1))])\n",
    "classifier = nn.Sequential(od)\n",
    "model.classifier = classifier\n",
    "\n",
    "# Displaying:\n",
    "println(['Model'], header=True)\n",
    "println([f'{model_name}, ' + ('Pretrained ' if pretrained else 'Not Pretrained')])\n",
    "println(['Classifier'], header=True)\n",
    "print([f'{layer}' for layer in od.keys()])\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading a Model Checkpoint'''\n",
    "\n",
    "\n",
    "println([('looking for \\'.pth\\' files in the default checkpoints folder; \\\n",
    "will load the latest one, but if none were found it is still OK...', 'y')])\n",
    "\n",
    "if os.path.exists(consts.checkpoints_path):\n",
    "    latest_checkpoint = latestCheckpoint()\n",
    "    if latest_checkpoint == None:\n",
    "        print('no \\'.pth\\' files were found.')\n",
    "        print(f'no checkpoint loaded.')\n",
    "    else:\n",
    "        model = loadCheckpoint(latest_checkpoint, weights)\n",
    "        print(f'checkpoint loaded from: \\\"{latest_checkpoint}\\\"')\n",
    "else:\n",
    "    print(f'no checkpoints directory found, created a new one.')\n",
    "    print(f'no checkpoint loaded.')\n",
    "    os.mkdir(consts.checkpoints_path)\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Defining the Training Hyperparameters'''\n",
    "\n",
    "\n",
    "# Hyperparameters:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 10\n",
    "learning_rate = .001\n",
    "criterion = nn.NLLLoss()\n",
    "# Only train the CLASSIFIER parameters, FEATURE parameters are frozen!\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr = learning_rate)\n",
    "# Casting the model instance to the available hardware:\n",
    "model.to(device)\n",
    "# Hyperparameters names for displaying:\n",
    "device_name = ('GPU' if device == torch.device('cuda') else 'CPU')\n",
    "criterion_name = 'Negative Log Loss'\n",
    "optimizer_name = 'Adam'\n",
    "# Displaying:\n",
    "println(['model', 'pretrained', 'device', 'epochs'], header=True)\n",
    "println([model_name, ('yes' if pretrained else 'no'), \n",
    "        (device_name, ('g' if device_name == 'GPU' else 'r')), epochs])\n",
    "println(['learning rate', 'loss function', 'optimizer'], header=True)\n",
    "println([learning_rate, criterion_name, optimizer_name])\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Model Training, Validation, and Testing'''\n",
    "\n",
    "\n",
    "println([(f'training on {device_name} started, might take a few minutes to complete...', 'y')])\n",
    "\n",
    "# Training and validating part, displaying too:\n",
    "train_metadata = []\n",
    "start_training_time = time.time()\n",
    "println(['epoch', 'time', 'train loss', 'valid loss', 'accuracy'], header=True)\n",
    "\n",
    "for idx in range(epochs):\n",
    "    # Keep the model object up-to-date (because we send it to another function):\n",
    "    hyperparams = (model, optimizer, device, criterion)\n",
    "    # Epoch metadata:\n",
    "    start_time = time.time()\n",
    "    end_time = None\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    accuracy = 0\n",
    "    # Switching model mode to TRAINING.\n",
    "    # Training the model using the entire train image set:\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:     \n",
    "        train_loss += train(hyperparams, inputs, labels)\n",
    "    # Switching model mode to EVALUTAION.\n",
    "    # Validating the model using the entire valid image set:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            loss, acc = test(hyperparams, inputs, labels)\n",
    "            valid_loss += loss\n",
    "            accuracy += acc\n",
    "    end_time = time.time()\n",
    "    aggregated_metadata = (idx, start_time, end_time, train_loss, valid_loss, accuracy, (train_loader, valid_loader))\n",
    "    # Collect this epoch's metadata and add to the list list:\n",
    "    collect(train_metadata, aggregated_metadata)\n",
    "    # Display this epoch's metadata:\n",
    "    displayTrain(train_metadata, idx)\n",
    "\n",
    "end_training_time = time.time()\n",
    "total_training_time = (end_training_time - start_training_time )\n",
    "\n",
    "# Displaying the collected training metadata:\n",
    "println([(f'training finished, results:', 'y')])\n",
    "displayTrain(train_metadata)\n",
    "\n",
    "\n",
    "# Testing part:\n",
    "println([(f'testing the trained model:', 'y')])\n",
    "test_loss = 0\n",
    "accuracy = 0\n",
    "# Testing loop:\n",
    "model.eval()\n",
    "hyperparams = (model, optimizer, device, criterion)\n",
    "for inputs, labels in test_loader:\n",
    "    loss, acc = test(hyperparams, inputs, labels)\n",
    "    test_loss += loss\n",
    "    accuracy += acc\n",
    "displayTest(test_loss, accuracy, test_loader)\n",
    "\n",
    "println([('done.', 'g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Saving a Model Checkpoint'''\n",
    "\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(consts.checkpoint_timestamp_format)\n",
    "checkpoint_name = f'{timestamp}.pth'\n",
    "model.class_to_idx = train_data.class_to_idx\n",
    "checkpoint = {'network': 'vgg16',\n",
    "              'input_size': in_features,\n",
    "              'output_size': len(classes), \n",
    "              'batch_size': batch_size,\n",
    "              'classifier' : classifier,\n",
    "              'epochs': epochs,\n",
    "              'optimizer': optimizer.state_dict(),\n",
    "              'state_dict': model.state_dict(),\n",
    "              'class_to_idx': model.class_to_idx}\n",
    "checkpoint_path = consts.checkpoints_path + '/' + checkpoint_name\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f'checkpoint saved to \\\"{checkpoint_path}\\\"')\n",
    "\n",
    "println([('done.', 'g')])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
