{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a4dfb1-584f-4c6e-b39a-c32fc1863ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mdone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Importing Modules'''\n",
    "\n",
    "# vanilla:\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "# external:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models\n",
    "import git\n",
    "import splitfolders\n",
    "from dotenv import load_dotenv\n",
    "# custom:\n",
    "from routines import *\n",
    "from displays import *\n",
    "import myTransforms\n",
    "import consts\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6032a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mdone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Parsing the .env File'''\n",
    "\n",
    "# Loading sensitive info from the dotenv file.\n",
    "# It is needed in order to clone the data repo:\n",
    "if not load_dotenv(consts.dotenv_path):\n",
    "    e_msg = 'cannot find the required .env file'\n",
    "    raise SystemExit(e_msg)\n",
    "gh_token = os.getenv('GH_TOKEN')\n",
    "gh_username = os.getenv('GH_USERNAME')\n",
    "repo_name = os.getenv('REMOTE_REPO_NAME')\n",
    "repo_url = f'https://{gh_token}@github.com/{gh_username}/{repo_name}.git'\n",
    "repo_dir_path = f'./{repo_name}'\n",
    "dataset_dir_path = repo_dir_path + '/dataset'\n",
    "classes_file_path = repo_dir_path + '/classes.json'\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5fa87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mdone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Cloning the Remote Data-Repository'''\n",
    "\n",
    "url_issue = False\n",
    "# Checks if a leftover repo exists, overwrite it if so:\n",
    "if os.path.exists(repo_dir_path):\n",
    "    git.rmtree(repo_dir_path)\n",
    "# Clones the repo, and raises an exception if the remote URL is corrupted:\n",
    "try:\n",
    "    git.Repo.clone_from(repo_url, repo_name)\n",
    "except Exception as e:\n",
    "    url_issue = True\n",
    "    pass\n",
    "if url_issue:\n",
    "    e_msg = 'there is an issue with the remote repo URL'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21a22b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtotal classes       \u001b[0m\n",
      "\u001b[0m15                  \u001b[0m\n",
      "\n",
      "\u001b[1mimages per class    \u001b[0m\n",
      "\u001b[0m30                  \u001b[0m\n",
      "\n",
      "\u001b[32mdone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Parsing the JSON File from the Data Repository'''\n",
    "\n",
    "json_not_found = False\n",
    "try:\n",
    "    with open(classes_file_path, 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "        classes = OrderedDict(json_file[0])\n",
    "        images_per_class = json_file[1]['images_per_class']\n",
    "        # Creates a list of all subdir names (strings) within dataset dir:\n",
    "        dir_names = [dataset_dir_path + '\\\\%.2d' % i for i in range(1, len(classes) + 1)]\n",
    "    # Displays the JSON file metadata:\n",
    "    print_matrix([\n",
    "        ('total classes', len(classes)),\n",
    "        ('images per class', images_per_class)\n",
    "        ], vector=True)\n",
    "except FileNotFoundError as e:\n",
    "    json_not_found=True\n",
    "if json_not_found:\n",
    "    e_msg=f'cannot locate the \\'classes.json\\' file in \"{repo_name}\".'\\\n",
    "        + f'\\nre-run the \\'Cloning the Remote Data-Repositoryg\\' cell and try again.'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a41e81-7b80-4b39-be79-c40461103f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mperforming a valdiation of the cloned data repo according its JSON file before any further training can take place...\u001b[0m\n",
      "\u001b[1mid                  \u001b[1mparsed class        \u001b[1mimages found        \u001b[0m\n",
      "\u001b[0m1                   \u001b[0mAustralia           \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m2                   \u001b[0mBrazil              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m3                   \u001b[0mCanada              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m4                   \u001b[0mChina               \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m5                   \u001b[0mFrance              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m6                   \u001b[0mGermany             \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m7                   \u001b[0mIndia               \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m8                   \u001b[0mIsrael              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m9                   \u001b[0mItaly               \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m10                  \u001b[0mJapan               \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m11                  \u001b[0mRussia              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m12                  \u001b[0mSpain               \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m13                  \u001b[0mSweden              \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m14                  \u001b[0mUK                  \u001b[32m30                  \u001b[0m\n",
      "\u001b[0m15                  \u001b[0mUSA                 \u001b[32m30                  \u001b[0m\n",
      "\u001b[1m                    \u001b[1m                    \u001b[1mtotal images        \u001b[0m\n",
      "\u001b[0m                    \u001b[0m                    \u001b[32m450                 \u001b[0m\n",
      "\u001b[32mdone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Validating the Dataset Directory'''\n",
    "\n",
    "msg = 'performing a valdiation of the cloned data repo according its JSON file \\\n",
    "before any further training can take place...'\n",
    "print_msg(msg)\n",
    "\n",
    "files_per_class = []\n",
    "bad_dirs = []\n",
    "json_ne_dirs = False\n",
    "\n",
    "# Validates the number of classes defined in the JSON equals to number of classes subdirs:\n",
    "if len(os.listdir(dataset_dir_path)) != len(classes):\n",
    "    json_ne_dirs=True\n",
    "\n",
    "# Validates that the number of images in each class subdir equals to the one defined in the JSON:\n",
    "for dir_tuple in os.walk(dataset_dir_path):\n",
    "    if dir_tuple[0] in dir_names: # skips junk directories\n",
    "        images_in_dir = len(dir_tuple[2])\n",
    "        files_per_class.append(images_in_dir)\n",
    "        if images_in_dir != images_per_class:\n",
    "            bad_dirs.append(dir_tuple[0])\n",
    "\n",
    "# Raise exceptions if needed:\n",
    "if json_ne_dirs:\n",
    "    e_msg=f'number of classes according to the JSON file ({len(classes)})'\\\n",
    "        + f' does not correlate with total dirs ({len(os.listdir(dataset_dir_path))})'\\\n",
    "        + f' in \\\"{dataset_dir_path}\\\".'\\\n",
    "        + f'\\nre-run \\'Data Repository Cloning\\' cell then re-run this cell.'\n",
    "    raise SystemExit(e_msg)\n",
    "elif bad_dirs != []:\n",
    "    e_msg=f'image count in the following directories is incorrect: {bad_dirs}'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "# Displaying...\n",
    "# If the number of files found in a class subdir does not strictly equal\n",
    "#  to the defined number (from the JSON file), the number will be highlighted\n",
    "#  with red color; elsewise, in green.\n",
    "\n",
    "print_header(['id', 'parsed class', 'images found'])\n",
    "for idx, (id, Class) in enumerate(classes.items()):\n",
    "    print_line([id,\n",
    "            Class.upper() if Class in ['uk','usa'] else Class.capitalize(),\n",
    "            (files_per_class[idx], ('g' if files_per_class[idx] == images_per_class else 'r'))])\n",
    "print_header(['','','total images'])\n",
    "print_line(['','',(sum(files_per_class), ('g' if (sum(files_per_class) == (len(classes) * images_per_class)) else 'r'))])\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a462ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mcreating a new 'sets' dir, with three subdirs of images: train', 'valid', 'test'...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 450 files [00:01, 251.67 files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mdone.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Splitting the Dataset'''\n",
    "\n",
    "msg = 'creating a new \\'sets\\' dir, with three subdirs of images: train\\', \\'valid\\', \\'test\\'...'\n",
    "print_msg(msg)\n",
    "\n",
    "# Deleting a leftover 'sets' directory if such exists:\n",
    "\n",
    "if os.path.exists(consts.sets_path):\n",
    "    shutil.rmtree(consts.sets_path)\n",
    "\n",
    "# Randomly splitting the dataset into 'test', 'valid', 'test' image directories:\n",
    "splitfolder_issue = False\n",
    "try:\n",
    "    splitfolders.ratio(\n",
    "        dataset_dir_path,\n",
    "        output=consts.sets_path,\n",
    "        seed=1337,\n",
    "        ratio=(.8, .1, .1),\n",
    "        group_prefix=None,\n",
    "        move=False)\n",
    "except:\n",
    "    splitfolder_issue = True\n",
    "if splitfolder_issue:\n",
    "    e_msg = 'exception raised within a splitfolders.ratio() call'\n",
    "    raise SystemExit(e_msg)\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83746019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is 32\n",
      "\u001b[32mdone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Creating DataLoaders'''\n",
    "\n",
    "batch_size = 32\n",
    "print(f'batch size is {batch_size}')\n",
    "\n",
    "# Instanciating each set:\n",
    "train_data = datasets.ImageFolder(consts.trainset_path, transform=myTransforms.train_transforms)\n",
    "valid_data = datasets.ImageFolder(consts.validset_path, transform=myTransforms.valid_transforms)\n",
    "test_data = datasets.ImageFolder(consts.testset_path, transform=myTransforms.test_transforms)\n",
    "\n",
    "# Creating a DataLoader for each set:\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ffdceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mModel               \u001b[0m\n",
      "\u001b[0mVGG16, Pretrained   \u001b[0m\n",
      "\n",
      "\u001b[1mClassifier          \u001b[0m\n",
      "\u001b[0m['fc1', 'drop', 'relu', 'fc2', 'output']\u001b[0m\n",
      "\n",
      "\u001b[32mdone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Instaciating a Model and a Classifier'''\n",
    "\n",
    "pretrained = True\n",
    "weights=('DEFAULT' if pretrained else None)\n",
    "model = models.vgg16(weights=weights)\n",
    "model_name = 'VGG16'\n",
    "for param in model.parameters():\n",
    "    # Freeze the MODEL parameters so we don't backprop through them! Only through the classifier.\n",
    "    param.requires_grad = False\n",
    "dropout_probability = .5\n",
    "in_features = 25088\n",
    "out_features = 1024\n",
    "od = OrderedDict([('fc1', nn.Linear(in_features, out_features)),\n",
    "                ('drop', nn.Dropout(p=dropout_probability)),\n",
    "                ('relu', nn.ReLU()),\n",
    "                ('fc2', nn.Linear(out_features, len(classes))),\n",
    "                ('output', nn.LogSoftmax(dim=1))])\n",
    "classifier = nn.Sequential(od)\n",
    "model.classifier = classifier\n",
    "\n",
    "# Displaying:\n",
    "print_matrix(\n",
    "    [\n",
    "        ('Model', f'{model_name}, ' + ('Pretrained ' if pretrained else 'Not Pretrained')),\n",
    "        ('Classifier', str([f'{layer}' for layer in od.keys()])),\n",
    "    ],\n",
    "    vector=True)\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e6f8651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mlooking for '.pth' files in the default checkpoints folder;\n",
      "will load the latest one found, but if none were found it is still OK...\u001b[0m\n",
      "no checkpoints directory found, created a new one.\n",
      "no checkpoint loaded.\n",
      "\u001b[32mdone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Loading a Model Checkpoint'''\n",
    "\n",
    "msg = 'looking for \\'.pth\\' files in the default checkpoints folder;\\n\\\n",
    "will load the latest one found, but if none were found it is still OK...'\n",
    "print_msg(msg)\n",
    "\n",
    "if os.path.exists(consts.checkpoints_path):\n",
    "    latest_checkpoint = latestCheckpoint()\n",
    "    if latest_checkpoint == None:\n",
    "        print('no \\'.pth\\' files were found.')\n",
    "        print(f'no checkpoint loaded.')\n",
    "    else:\n",
    "        model = loadCheckpoint(latest_checkpoint, weights)\n",
    "        print(f'checkpoint loaded from: \\\"{latest_checkpoint}\\\"')\n",
    "else:\n",
    "    print(f'no checkpoints directory found, created a new one.')\n",
    "    print(f'no checkpoint loaded.')\n",
    "    os.mkdir(consts.checkpoints_path)\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5681ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmodel               \u001b[0m\n",
      "\u001b[0mVGG16               \u001b[0m\n",
      "\n",
      "\u001b[1mpretrained          \u001b[0m\n",
      "\u001b[0myes                 \u001b[0m\n",
      "\n",
      "\u001b[1mdevice              \u001b[0m\n",
      "\u001b[0mGPU                 \u001b[0m\n",
      "\n",
      "\u001b[1mepochs              \u001b[0m\n",
      "\u001b[0m10                  \u001b[0m\n",
      "\n",
      "\u001b[1mlearning rate       \u001b[0m\n",
      "\u001b[0m0.001               \u001b[0m\n",
      "\n",
      "\u001b[1mloss function       \u001b[0m\n",
      "\u001b[0mNegative Log Loss   \u001b[0m\n",
      "\n",
      "\u001b[1moptimizer           \u001b[0m\n",
      "\u001b[0mAdam                \u001b[0m\n",
      "\n",
      "\u001b[32mdone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Hyperparameters'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 10\n",
    "learning_rate = .001\n",
    "criterion = nn.NLLLoss()\n",
    "# Only train the CLASSIFIER parameters, FEATURE parameters are frozen!\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr = learning_rate)\n",
    "# Casting the model instance to the available hardware:\n",
    "model.to(device)\n",
    "# Hyperparameters names for displaying:\n",
    "device_name = ('GPU' if device == torch.device('cuda') else 'CPU')\n",
    "criterion_name = 'Negative Log Loss'\n",
    "optimizer_name = 'Adam'\n",
    "# Displaying:\n",
    "print_matrix([\n",
    "        ('model', model_name),\n",
    "        ('pretrained', 'yes' if pretrained else 'no'),\n",
    "        ('device', device_name),\n",
    "        ('epochs', epochs),\n",
    "        ('learning rate', learning_rate),\n",
    "        ('loss function', criterion_name),\n",
    "        ('optimizer', optimizer_name)\n",
    "], vector=True)\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c4a97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mtraining on GPU started, might take a few minutes to complete...\u001b[0m\n",
      "\u001b[1mepoch               \u001b[1mtime                \u001b[1mtrain loss          \u001b[1mvalid loss          \u001b[1maccuracy            \u001b[0m\n",
      "\u001b[0m1                   \u001b[0m00:09               \u001b[0m5.713               \u001b[0m3.783               \u001b[0m0.302               \u001b[0m\n",
      "\u001b[0m2                   \u001b[0m00:05               \u001b[0m3.094               \u001b[0m1.495               \u001b[0m0.472               \u001b[0m\n",
      "\u001b[0m3                   \u001b[0m00:05               \u001b[0m1.770               \u001b[0m1.113               \u001b[0m0.720               \u001b[0m\n",
      "\u001b[0m4                   \u001b[0m00:05               \u001b[0m1.550               \u001b[0m0.836               \u001b[0m0.775               \u001b[0m\n",
      "\u001b[0m5                   \u001b[0m00:05               \u001b[0m1.330               \u001b[0m0.883               \u001b[0m0.728               \u001b[0m\n",
      "\u001b[0m6                   \u001b[0m00:05               \u001b[0m1.306               \u001b[0m0.722               \u001b[0m0.760               \u001b[0m\n",
      "\u001b[0m7                   \u001b[0m00:04               \u001b[0m1.127               \u001b[0m0.704               \u001b[0m0.798               \u001b[0m\n",
      "\u001b[0m8                   \u001b[0m00:04               \u001b[0m1.084               \u001b[0m0.420               \u001b[0m0.861               \u001b[0m\n",
      "\u001b[0m9                   \u001b[0m00:05               \u001b[0m0.974               \u001b[0m0.602               \u001b[0m0.775               \u001b[0m\n",
      "\u001b[0m10                  \u001b[0m00:05               \u001b[0m0.925               \u001b[0m0.410               \u001b[0m0.899               \u001b[0m\n",
      "\u001b[33mtraining finished, results:\u001b[0m\n",
      "\u001b[1mepochs              \u001b[1mtotal time          \u001b[1mmean train loss     \u001b[1mmean valid loss     \u001b[1mmean accuracy       \u001b[0m\n",
      "\u001b[0m10                  \u001b[0m00:52               \u001b[0m1.887               \u001b[0m1.097               \u001b[0m0.709               \u001b[0m\n",
      "\u001b[33mtesting the trained model:\u001b[0m\n",
      "\u001b[1m                    \u001b[1m                    \u001b[1mloss                \u001b[1m                    \u001b[1maccuracy            \u001b[0m\n",
      "\u001b[0m                    \u001b[0m                    \u001b[0m0.613               \u001b[0m                    \u001b[0m0.760               \u001b[0m\n",
      "\u001b[32mdone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Model Training, Validation, and Testing'''\n",
    "\n",
    "msg = f'training on {device_name} started, might take a few minutes to complete...'\n",
    "print_msg(msg)\n",
    "\n",
    "# Training and validating part, displaying too:\n",
    "train_metadata = []\n",
    "start_training_time = time.time()\n",
    "print_header(['epoch', 'time', 'train loss', 'valid loss', 'accuracy'])\n",
    "\n",
    "for idx in range(epochs):\n",
    "    # Keep the model object up-to-date (because we send it to another function):\n",
    "    hyperparams = (model, optimizer, device, criterion)\n",
    "    # Epoch metadata:\n",
    "    start_time = time.time()\n",
    "    end_time = None\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    accuracy = 0\n",
    "    # Switching model mode to TRAINING.\n",
    "    # Training the model using the entire train image set:\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:     \n",
    "        train_loss += train(hyperparams, inputs, labels)\n",
    "    # Switching model mode to EVALUTAION.\n",
    "    # Validating the model using the entire valid image set:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            loss, acc = test(hyperparams, inputs, labels)\n",
    "            valid_loss += loss\n",
    "            accuracy += acc\n",
    "    end_time = time.time()\n",
    "    aggregated_metadata = (idx, start_time, end_time, train_loss, valid_loss, accuracy, (train_loader, valid_loader))\n",
    "    # Collect this epoch's metadata and add to the list list:\n",
    "    collect(train_metadata, aggregated_metadata)\n",
    "    # Display this epoch's metadata:\n",
    "    print_epoch(train_metadata, idx)\n",
    "\n",
    "end_training_time = time.time()\n",
    "total_training_time = end_training_time - start_training_time\n",
    "\n",
    "# Displaying the collected training metadata:\n",
    "print_msg('training finished, results:')\n",
    "print_train_summary(train_metadata)\n",
    "\n",
    "\n",
    "# Testing part:\n",
    "print_msg('testing the trained model:')\n",
    "test_loss = 0\n",
    "accuracy = 0\n",
    "# Testing loop:\n",
    "model.eval()\n",
    "hyperparams = (model, optimizer, device, criterion)\n",
    "for inputs, labels in test_loader:\n",
    "    loss, acc = test(hyperparams, inputs, labels)\n",
    "    test_loss += loss\n",
    "    accuracy += acc\n",
    "print_test_summary(test_loss, accuracy, test_loader)\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d75cdd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to \"./checkpoints/121156_260223.pth\"\n",
      "\u001b[32mdone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''Saving a Model Checkpoint After Training'''\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(consts.checkpoint_timestamp_format)\n",
    "checkpoint_name = f'{timestamp}.pth'\n",
    "model.class_to_idx = train_data.class_to_idx\n",
    "checkpoint = {'network': 'vgg16',\n",
    "              'input_size': in_features,\n",
    "              'output_size': len(classes), \n",
    "              'batch_size': batch_size,\n",
    "              'classifier' : classifier,\n",
    "              'epochs': epochs,\n",
    "              'optimizer': optimizer.state_dict(),\n",
    "              'state_dict': model.state_dict(),\n",
    "              'class_to_idx': model.class_to_idx}\n",
    "checkpoint_path = consts.checkpoints_path + '/' + checkpoint_name\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f'checkpoint saved to \\\"{checkpoint_path}\\\"')\n",
    "\n",
    "print_msg('done.', 'g')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
